{"nbformat": 4, "nbformat_minor": 1, "cells": [{"source": "<div><img src=\"http://www.stevinsonauto.net/assets/Icon_Brake.png\", width=270, height=270, align = 'right'> \n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/51/IBM_logo.svg/640px-IBM_logo.svg.png\", width = 90, height = 90, align = 'right', style=\"margin:0px 25px\"></div>\n\n# Data Science for Automotive:  Classifying Brake Events\n\n_________________\n\nIn this lab you will explore braking data to differentiate between driver profiles using Python and Apache Spark.  Along the way you'll learn how to  manipulate, visualize, and ultimately model data sets in DSX.\n\n\n#### Table of Contents\n\n1. Problem Statement\n\n2. Load Data from IBM Object Storage\n\n3. Exploratory Data Analysis with SparkSQL and PixieDust\n\n4. Machine Learning with SparkML\n\n5. Conclusion\n\n______________________\n\n### Problem Statement\n\nThe service bays at dealerships have seen an increase in warranty claims related to brakes. However, it may not always make sense to honor a warranty claim.  Sometimes the brake issue may be due to vehicle malfunction; other times it can be due to aggressive driving style.  \n\n>Using historical telematics data of known driver types, can we classify the driving style of customers making warranty claims?  If so, we'll be able to provide a service to dealerships that allows them to classify the brake event and driver type for customers making a warranty claim.  \n\nTo do this you will need to analyze connected car data to discover the various patterns associated with different drivers.  *Please keep in mind that the hints often contain the solution to the exercise, so consider that before using them.*\n\n__Note:__  Before we dive in, double click on this text.  You'll notice that the cell has changed and you should see some formatting syntax.  This is a what a Markdown cell looks like before it is executed.  Now click on the play button in the menu bar above.  Voila!  Nicely rendered text.  If you accidentally double click on a Markdown cell this will happen, so just execute the cell again with the play button and it will render.  \n\nOk, let's get started!\n\n_______", "cell_type": "markdown", "metadata": {}}, {"source": "### Load Data\n\nThere are many ways to connect to data sources in DSX.  For this lab we will be using the \"Files/Connections\" panel located in the top right of your DSX console.  The icon for this panel is \"10/01\".  \n\nAfter opening the panel click on the \"Insert to code\" button for the `historical_brake_events.csv` data asset.  You should see a drop down menu with several options.  **Click on the code cell below** and then insert a SparkSession DataFrame using the drop down menu.", "cell_type": "markdown", "metadata": {}}, {"source": "## Insert SparkSession DataFrame in this cell using the Files/Connections feature in DSX\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": "What's happening in this code cell?  First, we are importing the library `ibmos2spark` (IBM Object Storage to Spark) which allows us to take a file in Object Storage and immediately push it to Spark.  Then we have the credentials for our Object Storage instance defined for us.  This is needed to provide access to the data set.  At the end of the cell we define the name of our Spark DataFrame (default is df_data_1) and take the first couple records to inspect them.  \n\nYou can execute the code in this cell by pressing the \"Play\" button in the Notebook menu above.  The output will be displayed immediately below the code cell.\n\n#### Check SparkSession\n\nBefore loading the data into Spark, a SparkSession had to be created.  Think of this as the gateway to many parts of the Spark API.  The default variable name for the SparkSession is simply `spark`.  Check the `spark` variable now and remember that it's there - this will be important later.", "cell_type": "markdown", "metadata": {}}, {"source": "## Check SparkSession by typing `spark` on the next line then clicking the 'play' button in the menu bar\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": "#### Rename the DataFrame\n\nWe will explore the data in just a moment, but first give the DataFrame a better name.  `df_data_1` is uninspired and doesn't help us keep track of what we're working with later.  The convention in Spark programming is to add a 'DF' at the end of the name to signify 'DataFrame'.\n<br><br>\n <div class=\"panel-group\" id=\"accordion-1\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse1-1\">\n        Hint</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-1\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\nYou can reassign a variable by using the `=` operator.  For example, the command \"renamedDF = df_data_1\" will let you work with the new variable `renamedDF`. </div>\n    </div>\n  </div>", "cell_type": "markdown", "metadata": {}}, {"source": "## Give `df_data_1` a more appropriate name for this project - something that better reflects the data set\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "____________\n\n### Exploratory Data Analysis\n\nA preview of the data set was shown when we loaded it from Object Storage.  The format - Row RDD - was a bit difficult to interpret visually.  There are several methods to display the data in a tabular format.\n\n`df_data_1.show()`\n\n`df_data_1.toPandas()`\n\nTry these now, but substitute `df_data_1` for your renamed DataFrame.\n<br><br>\n\n <div class=\"panel-group\" id=\"accordion-2\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse1-2\">\n        Advanced Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-2\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">You can limit the number of records shown in the `.show()` method by passing an additional parameter, `n = <int>`.  For example, `df_data_1.show(n = 10)` will display the first ten records.</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse2-2\">\n        Advanced Optional 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-2\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Unlike `.show()`, the `.toPandas()` method doesn't take any additional parameters.  However, you can call the `.limit()` method before calling `.toPandas()` to limit the number of rows displayed. These commands can be chained using the '.' syntax.</div>\n    </div>\n  </div>", "cell_type": "markdown", "metadata": {}}, {"source": "## Display your DataFrame as a table using one of the two methods above.\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": "Take some time to look at the data and become acquainted with the various fields.  You can also check the schema by using the `.printSchema()` method on the DataFrame.", "cell_type": "markdown", "metadata": {}}, {"source": "## Inspect the schema of your DataFrame\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": "Notice anything strange?  The data types are all coded as 'string'.  Before we can compute any statistics or aggregations we'll need to cast these into the proper types.  Welcome to data analysis, where the data is dirty and the work is cut out for you! \n\n#### SparkSQL\n\nThere are different ways to do this in Spark but perhaps the simplest and most intuitive way is to use SparkSQL.  While not entirely ANSI compliant, SparkSQL is a powerful way for SQL developers to work on big data.  Let's start with the basics and then work toward changing the types.\n\n##### Querying a Temporary View\n\nIn order to run queries in familiar SQL syntax you'll need to create a temporary view.  To do this, call the `.createOrReplaceTempView()` method on your existing DataFrame.  Note that the table name is one of the parameters and must be in quotes.\n<br><br>\n<div class=\"panel-group\" id=\"accordion-1\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-32\" href=\"#collapse1-32\">\n        Hint</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-32\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Try: df_data_1.createOrReplaceTempView(\"yourtablename\"), but replace 'yourtablename' with what you want to call your table. </div>\n    </div>\n  </div>\n</div>", "cell_type": "markdown", "metadata": {}}, {"source": "## Create the temporary view here\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "Good.  Now we can access the SparkSession (remember?) and run queries against the temporary table.\n\nThe syntax for this is,\n\n> `SparkSession.sql(\"*insert SQL statement here*\")`\n\nKeep in mind you are running queries against the **temp table** and not the DataFrame.  Try a simple `\"SELECT * FROM yourtablename\"` statement and see what is returned.  You may have to add a command that you've already learned to display the data. \n\n<br>\n <div class=\"panel-group\" id=\"accordion-31\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse1-31\">\n        Hint</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-31\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Try chaining `.show()` or `.toPandas()` after your `.sql()` method.\n      </div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-31\" href=\"#collapse2-31\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-31\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Try `spark.sql(\"SELECT * FROM tablenamehere\").limit(5).toPandas()`</div>\n    </div>\n  </div>\n</div>", "cell_type": "markdown", "metadata": {}}, {"source": "## Try a simple select statement using the SparkSession and display the results\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": "##### Casting Columns as Different Data Types\n\nNow that you've built a gateway to writing SQL statements in Spark, it's time to cast columns into the correct type.  For example, `column1` can be cast as a floating point type using the following statement:\n\n> `SELECT cast(column1 as float) FROM yourtablename`\n\nIn order to confirm that this worked you'll have to check the schema again.  \n<br>\n<br>\n <div class=\"panel-group\" id=\"accordion-5\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-5\" href=\"#collapse1-5\">\n        Hint</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-5\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Chain `.printSchema()` onto `.sql()`.</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-5\" href=\"#collapse2-5\">\n        Advanced Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-5\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Use <i>limit</i> (i.e. <i>limit 2</i>) within your SQL statement to limit the number of rows returned.   Use `.show()` to display the values in the console.</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-5\" href=\"#collapse3-5\">\n        Advanced Optional 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-5\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\n      `spark.sql(\"SELECT cast(brake_time_sec as float) FROM yourtablename LIMIT 2\").show()`<br></div>\n    </div>\n  </div>\n</div> \n", "cell_type": "markdown", "metadata": {}}, {"source": "## Convert one column to float and then print the schema to confirm the type has changed\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "Excellent.  **Now cast all variables without letters as the proper type and store the result in a new DataFrame.**  Variables with decimals should be `float`, while others can be `int`. \n<br><br>\n<div class=\"panel-group\" id=\"accordion-51\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-51\" href=\"#collapse1-51\">\n        Hint</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-51\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">You can assign the output of a SparkSQL command to a variable in the same way you renamed your original RDD.  For example, \n      <br><br> `cleanTypesDF = spark.sql(\"...\")` </div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-52\" href=\"#collapse1-52\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-52\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Try: \n      <br><br> `cleanTypesDF = spark.sql(\"SELECT cast(... as ...), cast(... as ...) FROM yourtablename\")` </div>\n    </div>\n  </div>\n  </div>\n\n", "cell_type": "markdown", "metadata": {}}, {"source": "## Use SparkSQL to create a DataFrame with the proper types and assign it to a new variable.\n## Note: If you place your SQL statement in triple quotes - \"\"\" \"\"\" - you can ignore the indentation requirements of Python, making your \n## statement easier to read.\n\n\n\n## After changing the types and putting them in a new DF, print the schema again\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "Your schema should now have `VIN`, `type`, and `road_type` as strings; `brake_time_sec` and `brake_distance_ft` as floats, and the rest as integers.  Update the temporary view with the correct types and continue to the next step.\n\n<br>\n<div class=\"panel-group\" id=\"accordion-21\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-21\" href=\"#collapse1-21\">\n        Hint</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-21\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">You can update the temporary view by using `.createOrReplaceTempView(\"yourtablename\")` on your new DataFrame with the proper types.</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-22\" href=\"#collapse1-22\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-22\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Try: \n      <br><br> `cleanTypesDF.createOrReplaceTempView(\"tablenamehere\")` </div>\n    </div>\n  </div>\n  </div>", "cell_type": "markdown", "metadata": {}}, {"source": "## Update the temporary view based on your cleanTypesDF\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "##### Summary Statistics\n\nWith the data converted to the proper type, we can now group and aggregate some of the fields to get a better understanding of what's going on.  \n\nAs an introduction to this technique let's start by taking a simple average of a few columns - `braking_score`, `travel_speed`, and `brake_distance_ft`.  This is done by including an aggregate function in your SQL statement.  For example, \n\n> `spark.sql(\"SELECT AVG(column1) as avg_column1 FROM yourtablename\")`\n\nNow it's your turn.  Try to get all averages returned in one statement.\n\n<br>\n<div class=\"panel-group\" id=\"accordion-54\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-54\" href=\"#collapse1-54\">\n        Hint</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-54\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Like selecting columns themselves, aggregate functions are separated by commas until you reach 'FROM ...'.</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-55\" href=\"#collapse1-55\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-55\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Try: \n      <br><br> `spark.sql(\"SELECT AVG(braking_score), AVG(travel_speed), AVG(braking_distance) FROM yourtablename\")` </div>\n    </div>\n  </div>\n  </div>", "cell_type": "markdown", "metadata": {}}, {"source": "## Get the averages here.\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "We can also get the min, max, count, and other values from each column using aggregate functions.  More importantly, to better understand the variation between different brake events group the data by the `type` column and _then_ use aggregate functions.  This will provide a more meaningful result for our purposes.  \n\nTry using the GROUP BY clause now to group the data by `type` and find the AVG `braking_score` and SUM of `abs_event`.\n\n<br>\n<div class=\"panel-group\" id=\"accordion-70\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-70\" href=\"#collapse1-70\">\n        Hint</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-70\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">'GROUP BY' statements are placed after specifying the table being queried. </div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-71\" href=\"#collapse1-71\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-71\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Try: \n      <br><br> `spark.sql(\"SELECT type, AVG(braking_score), SUM(abs_event) FROM yourtablename GROUP BY type\").toPandas()`\n </div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-72\" href=\"#collapse1-72\">\n        Advanced Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-72\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\n      To improve the column names of your results, provide an alias for the aggregate fields.  You can do this by adding \"... as avg_score,\" when selecting a column.  For example, <br> <br>\n      `\"SELECT AVG(column1) as avgValue FROM yourtablename\"`\n </div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-78\" href=\"#collapse1-78\">\n        Advanced Optional 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-78\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Explore the dataset using other aggregate functions besides AVG and SUM.  What do you find?\n </div>\n    </div>\n  </div>\n  </div>", "cell_type": "markdown", "metadata": {}}, {"source": "## Select the average braking_score and sum of abs_events from your table, but make sure to group the data\n## by type.\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "## Empty cell for Advanced Option 2 \n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "What do you see?  What sort of conclusions would draw from these summary statistics?\n\nBesides having a brake event type (the `type` column) there is a `road_type` column as well.  **For the final exercise of this section, group the data by type _and_ road type, then compute the same statistics.  This time include average `brake_time_sec`, `brake_distance_ft` and `travel_speed`.**\n<br><br>\n<div class=\"panel-group\" id=\"accordion-57\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-57\" href=\"#collapse1-57\">\n        Hint</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-57\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">You can group by multiple columns by separating them with a comma.  For example, <br><br>\n      `\"SELECT column1, column2, column3 FROM tablenamehere GROUP BY column1, column2\"`</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-58\" href=\"#collapse1-58\">\n        Advanced Option</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-58\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Try finding the sum of each `brake_pressure` field in your groups.  Does this tell you anything?\n </div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-60\" href=\"#collapse1-60\">\n        Advanced Option 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-60\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\n      Sort your result by adding an `ORDER BY` clause after the `GROUP BY` clause.  You can pick `type` or `road type`.\n </div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-59\" href=\"#collapse1-59\">\n        Advanced Option 3</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-59\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\n      Create a new column in your data set (not the aggregated one) that displays the ratio between brake distance and brake time.  No hints here!\n </div>\n    </div>\n  </div>\n  </div>", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "## Create a more complete summary stats table, this time with the data grouped by type and road type\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "## Empty cell for Advanced Option 3\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "This table has some really informative descriptive statistics in it. You should be able to draw some preliminary conclusions about how the different brake events express themselves in the various columns.  Save this aggregate table in a new DataFrame because you're going to use it for plotting.  Remember to leave out any display methods like `.show()` - you just want the DataFrame.", "cell_type": "markdown", "metadata": {}}, {"source": "## Run the same command as before, but this time save it in a new DF, 'aggDF'.\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "________\n\n#### Data Visualization  with PixieDust\n\nData visualization in Python can be done but there is somewhat of a steep learning curve.  Luckily, some enterprising folks at IBM Watson Data Lab have created an easier alternative called PixieDust.  Even better, **IBM has open sourced this code and made the library available to everyone.**  The following cells are taken straight from the PixieDust tutorial and will make sure the library works in this Notebook.\n\nInstall PixieDust by accessing the UNIX system behind this Notebook with the '!' operator.  This will make sure the latest version is on the system.", "cell_type": "markdown", "metadata": {}}, {"source": "## To confirm you have the latest version of PixieDust on your system, run this cell\n!pip install --user --upgrade pixiedust", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "Now that you have PixieDust installed and up to date on your system, import the library into the Notebook.  Don't worry if you see a warning - as long as your version is greater than 1.0.2 you are good to go.  Also, now is a good time to import the `bokeh` and `seaborn` libraries because they are much prettier than the default `matplotlib`.  Thank me later.", "cell_type": "markdown", "metadata": {}}, {"source": "import pixiedust\nimport bokeh\nimport seaborn", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "To create a visualization all you have to do is pass a DataFrame to the `display()` function and PixieDust will output an inline, interactive graphical display that you can configure based on what you want to see.  Here's the basic command:\n\n> `display(myDF)`\n\nIt's that easy.  Try passing a DataFrame created by one of your queries to that function and see what happens!\n\n> `display(spark.sql(\"...\"))`\n\nThis will likely give you a table of the resulting DataFrame, but clicking on the 'Chart' icon in the top left corner allows you to select which variables and plot type to render.  Make sure you select 'Bokeh' as the rendering option on the right.\n\nRemember that we want to understand how variables express themselves across event types.  Try to come up with some visualizations that give insight into the data.  Here's a tip: **Keys** are your X-axis and **Values** are your Y-axis.\n\n<br>\n <div class=\"panel-group\" id=\"accordion-62\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse1-62\">\n        Hint</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-62\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\n      Try a scatter plot using `bokeh` with `brake_distance_ft` as value and `brake_time_sec as key`.  What happens if you color the data by type?  What pattern do you see?</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-62\" href=\"#collapse2-62\">\n       Advanced Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-62\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Discover the breakdown of ABS events by type.  What does this tell you?<br></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-62\" href=\"#collapse3-62\">\n        Advanced Optional 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-62\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Try coming up with another creative way to visualize the data beyond bar and scatter plots.  Maybe try plotting some of the data from `aggDF`.  If you find something cool, share it with others!  <br></div>\n    </div>\n  </div>\n</div> ", "cell_type": "markdown", "metadata": {}}, {"source": "## Pass a DataFrame to the `display()` function from PixeDust here.\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "pixiedust": {"displayParams": {"aggregation": "SUM", "kind": "reg", "mpld3": "false", "charttype": "grouped", "clusterby": "road_type", "rendererId": "bokeh", "orientation": "horizontal", "handlerId": "barChart", "keyFields": "type", "sortby": "Values ASC", "valueFields": "brake_distance_ft", "color": "type", "rowCount": "500", "chartsize": "73"}}}}, {"source": "When you've finished exploring the data visually and have begun to draw some conclusions, record them in the Markdown cell immediately below this one.  Simply click inside the cell and you can edit the text inside.  You can write a paragraph or you can use the list format that I've prepared.  Similar to a code cell, you render the Markdown by pressing 'Play'.", "cell_type": "markdown", "metadata": {}}, {"source": "##### Double Click Here to Add Your Conclusions\n\n- Conclusion A:\n\n- Conclusion B:\n\n- Conclusion C:", "cell_type": "markdown", "metadata": {}}, {"source": "_________\n<br>\nIf you've made it this far there are several things that should have become clear from the data.  To avoid spoilers I have hidden the obvious conclusions in the 'spoiler' tab below.  Once you've thought about it feel free to open up the tab and compare notes.\n\n<br>\n <div class=\"panel-group\" id=\"accordion-66\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse1-66\">\n        Conclusions <b>(**Spoilers!**)</b></a>\n      </h4>\n    </div>\n    <div id=\"collapse1-66\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Quality drivers have brake events with longer brake times, distances, and higher brake scores.  Aggressive drivers are the opposite.  Distracted drivers fall somewhere in between but have a noticeably higher number of ABS events.  These results should approximate what one would expect - distracted drivers probably slam on the brakes more often, aggressive drivers drive faster and brake more quickly, while quality drivers keep their distance and are gentler on the brake system.  </div>\n    </div>\n  </div>\n</div>\nHaving arrived at these conclusions as a result of exploratory data analysis and data visualization, we can now proceed to modeling the relationship between brake event type and the rest of the variables.\n\n_____\n\n### Machine Learning\n\nTo make this exercise simpler, in this section you will simply need to specify the fields you want to include in your model and then execute the code cells as per the instructions. \n\n#### Classification vs. Regression\n\nThe way you frame a problem in machine learning often determines model choice, and problems can be characterized as one of either regression or classification.  When the variable you want to understand or predict is continuous or numeric (such as price, braking score, etc.) it is generally considered a regression problem.  However if you want to classify a data point as belonging to a particular group (such as aggressive, quality, or distracted) then it makes more sense to use a classification model.  In this exercise we will use a variant of the decision tree model - Random Forest - for classification.\n\n#### SparkML\n\nApache Spark has a rich set of algorithms and data transformations included in the API that can be used for machine learning.  That's the great part!  The tricky part is that the models are built to accept the data in a particular format so some additional data preparation is required.  \n\n##### Feature Selection\n\nThe first thing we are going to do is select the columns - features - we want to include in our model.  These features will be modeled against the variable we want to understand, also referred to as the label.  In other words we are going to provide the model with a list of features and their corresponding label (quality, aggressive, etc.).  It will learn the relationship between each feature and the different labels, allowing us to make predictions as to what the label should be given new values for the features.\n\nIn the code cell below, specify the names of the columns you want to include as your **features**.  Store the result in a new variable, `inputColumns`.\n\n**Note:  For the sake of simplicity, leave out any columns of datatype 'string' from your features.**\n\n<br>\n <div class=\"panel-group\" id=\"accordion-67\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse1-67\">\n        Hint</b></a>\n      </h4>\n    </div>\n    <div id=\"collapse1-67\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Try: <br>\n      `inputColumns = [\"brake_distance_ft\", \"brake_time_sec\", \"abs_event\"]` <br>\n      Make sure to include any columns you think affect the event type.</div>\n    </div>\n  </div>\n</div>", "cell_type": "markdown", "metadata": {}}, {"source": "## Specify columns to include here.  If you need to check the columns use .printSchema() on your cleanTypesDF\n## use the format[\"col1\", \"col2\", \"col3\"] etc...\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "##### Transformations\n\n> _VectorAssembler_\n\nRecall how I mentioned some of these transformations could be tricky?  We are going to perform two.  First, we'll consolidate all of our features from their respective columns into a single vector.  A features vector. \n\nFill in the blank space in the code cell below with your input columns variable name, then run the cell to see a sample output of what a features vector looks like.", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer\n\nassembler = VectorAssembler(\n    inputCols = putInputColumnVariableHere,  ## Add your input column variable here\n    outputCol=\"features\")\n\n## Use the 'transform' method to build the features vector, then see a couple rows from the 'type' and new 'features' columns.\n## Make sure you are doing this on your DataFrame with the proper data types (i.e., cleanTypesDF)\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "> _StringIndexer_\n\nNext, we'll transform our `type` column - the one we want to be able to classify or predict.  Since it's a string, we'll have to convert it to another data type in order to use it with the implementation of Random Forest in Spark.\n\n<br>\n <div class=\"panel-group\" id=\"accordion-8\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-8\" href=\"#collapse1-8\">\n        Advanced Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-8\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Transform the `cleanTypesDF` with your StringIndexer, then select the type and indexedLabel columns and send the resulting DataFrame to Pandas to display results in a table.<br></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-8\" href=\"#collapse2-8\">\n        Advanced Optional 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-8\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">You can try predicting another string type column, 'road_type', and see what kind of results you get.</div>\n    </div>\n  </div>\n </div>", "cell_type": "markdown", "metadata": {}}, {"source": "## Make sure you are fitting the StringIndexer on your DataFrame with the proper data types!\nlabelIndexer = StringIndexer(inputCol=\"type\", outputCol=\"indexedLabel\").fit(cleanTypesDF)\n\n## Display the results of this transformation with Pandas\nlabelIndexer.transform(cleanTypesDF).select(\"type\", \"indexedLabel\").limit(5).toPandas()", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "With the transformations properly configured, you are in a position to build the Random Forest classifier. \n\n##### Pipelines\n\nIn SparkML, a [pipeline](https://spark.apache.org/docs/latest/ml-pipeline.html) is the logical flow of transformations and model selection to generate an output.  Much like you needed two transformers, other workflows may include multiple algorithms with their own transformations to arrive at a final output.  Think of the ML Pipeline as a cleaner, streamlined API for machine learning programs.  If you are interested in doing machine learning with Spark, the Pipelines construct is worth exploring.\n\nFirst define the model and its parameters, then wrap the transformations and model in a Pipeline:", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\n\n## Define model and its parameters.\nrf = RandomForestClassifier(labelCol = \"indexedLabel\", featuresCol = \"features\")\n\n## Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages = [assembler, labelIndexer, rf])", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "Before fitting the model to the data, split it up into training and test sets.  We'll train the model on 70% of the data, then use the other 30% to test the accuracy.", "cell_type": "markdown", "metadata": {}}, {"source": "## If your DataFrame with proper types isn't `cleanTypesDF`, make sure to insert the correct DF name here.\n(trainingDF, testDF) = cleanTypesDF.randomSplit([0.7, 0.3])\n\nprint \"Rows in training data:\", trainingDF.count()\nprint \"Rows in test data:\", testDF.count()", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "## Fit the pipeline on the training DataFrame.  This could take a minute or two while the model is trained.\n\nRFmodel = ", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "Alright!  Your random forest classifier is now ready to make predictions.  \n\n#### Prediction and Evaluation\n\nUse the test data set to estimate the accuracy of your model.  The concept here is that the model has generalized the relationship between your labels and the features, but only based on the data points you provided. Feeding the model new rows of data it has never seen before and then checking to see if its predictions were correct will give you a sense of how well it has captured the relationship between the features and their labels.\n\n<br>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-99\" href=\"#collapse2-99\">\n        Advanced Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-99\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">See if you can show the predicted types alongside the actual types and the features column.  Use Pandas to display the table nicely.</div>\n    </div>\n  </div>\n </div>\n", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n## Predict on test set\npredictionsDF = RFmodel.transform(testDF)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\")\naccuracy = evaluator.evaluate(predictionsDF)\n\n## Show test error rate\nprint(\"Test Error = %g\" % (1.0 - accuracy))", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "## Cell for Advanced Optional\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "You should have achieved a fairly accurate result.  Nicely done!  \n_____\n\n### Export Pipeline to Object Storage\n\nIt's one thing to build a model and it's quite another to operationalize it.  In this case the business may want to provide an application that service bays can use to check the recent driving behavior of customers.  Imagine providing customized service at the dealership based on the driving tendencies of the customer.  The possibilities for improving customer experience and increasing profit are real, but first you need to make this predictive model available to others in your organization. \n\nAs your final task, save your pipeline to the UNIX system then push it to Object Storage.\n\n##### Save and Zip Pipeline\n\nWhen you save your pipeline it will create a directory in the local file system on DSX.  Before we can put it in Object Storage we'll have to zip the contents into a single file.\n\nSaving a model or pipeline in Spark is pretty easy.  Use the `.save(\"./modelname\")` method on your pipeline and it will be written to the system.", "cell_type": "markdown", "metadata": {}}, {"source": "## Save your pipeline and model to the system\n\nRFmodel.save(\"./brakeEventModel\")", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "Now run the next cell to zip the directory and its contents into a single file.  This will generate a fair amount of output and could take a minute.", "cell_type": "markdown", "metadata": {}}, {"source": "## Zip the directory\n\n!tar -zcvf RFmodel.tar.gz brakeEventModel/  ", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "Check the local file system for the presence of your zipped file using `!ls` or `!ls -ls`.", "cell_type": "markdown", "metadata": {}}, {"source": "## Confirm model was written to local file system here.\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "##### Define Your Object Storage Credentials\n\nIn your Files/Connections tab, click on 'insert to code' to display the drop down menu.  You should see an option to 'insert credentials'.  These are the credentials to your Object Storage instance.  Add them in the cell below, run it, then run the following cell to define the function that will put the files in Object Storage.", "cell_type": "markdown", "metadata": {}}, {"source": "## Insert credentials in this cell.\n## Make sure the 'filename' at the end matches the zipped file you just created!!\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "## All you have to do here is run the cell - no changes needed.\n## Define the function to put a file in Object Storage\nfrom io import BytesIO  \nimport requests  \nimport json  \n\ndef put_file(credentials, local_file_name):  \n    \"\"\"This functions returns a StringIO object containing\n    the file content from Bluemix Object Storage V3.\"\"\"\n    f = open(local_file_name,'r')\n    my_data = f.read()\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']},\n            'password': credentials['password']}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', credentials['container'], '/', local_file_name])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.put(url=url2, headers=headers2, data = my_data )\n    print resp2", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "Now use the `put_file()` function and simply pass your credentials and the filenames of your pipeline and model.  A 'Response [201]' output indicates that the write was successful.\n\n<br>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-93\" href=\"#collapse2-93\">\n        Hint</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-93\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">The `put_file()` function takes two parameters - credentials and the name of the file in the local file system.  \n      <br>Try: <br>\n      `put_file(credentials_1, \"modelname.tar.gz\")`</div>\n    </div>\n  </div>\n </div>", "cell_type": "markdown", "metadata": {}}, {"source": "## Use this cell to put your zipped file containing the model into Object Storage.\n\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "____\n\n## Conclusion\n\nWell done!  You've built a Random Forest classifier that can accurately predict the type of brake event based on features surrounding braking distance, time, and others.  This model can help better understand the tendencies of various drivers and optimize operations in different parts of the automotive business.  A real world data set will likely not be as straightforward as the one you worked with here (it almost certainly won't).  However, the methodology and basic techniques have been laid out to help you on your way towards building data products infused with machine learning.  \n\nBefore you go, please take a moment to review the material that you have completed in this lab!  Going through the workflow one more time should help concretize the terms, flow, and methodology that you learned.\n\n- **Problem Statement:**\n    * Defined the question being asked of the data\n\n\n- **Load Data from IBM Object Storage:**\n    * Created Spark DataFrames, renamed them, and checked the SparkSession\n\n\n- **Exploratory Data Analysis with SparkSQL and PixieDust:**\n    - View schemas and display DataFrames with Pandas\n    - Create temporary views and query them\n    - Clean up data types with SQL statements\n    - Create new DataFrames with aggregate functions\n    - Visualized data with PixieDust\n    - Summarized conclusions in Markdown\n\n\n- **Machine Learning with SparkML:**\n    - Selected features to learn relationship to labels\n    - Built ML Pipeline with VectorAssembler, StringIndexer and Random Forest\n    - Predicted on unseen data and evaluated accuracy\n    - Exported model to Object Storage for future use\n\n_____\n\n#### Additional Resources\n\n[Official Apache Spark Documentation](https://spark.apache.org)\n\n\nQuestions?  Contact <rafi.kurlansik@ibm.com> or tweet me @kurlare.<br>\nInterested in learning more?  Explore the Community Tiles in DSX for more tutorials and data sets.  Or, head over to [CognitiveClass.ai](http://www.cognitiveclass.ai/) for free classes on Apache Spark, machine learning, and more.\n_______\n\n\n<div><br><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/51/IBM_logo.svg/640px-IBM_logo.svg.png\" width = 200 height = 200>\n</div><br>", "cell_type": "markdown", "metadata": {}}], "metadata": {"language_info": {"pygments_lexer": "ipython2", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 2}, "version": "2.7.11", "name": "python", "file_extension": ".py", "nbconvert_exporter": "python"}, "kernelspec": {"language": "python", "display_name": "Python 2 with Spark 2.0", "name": "python2-spark20"}}}